<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Krishnamurthy (Dj) Dvijotham</title>
    <link rel="stylesheet" href="new_styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1 class="profile-name">Krishnamurthy (Dj) Dvijotham</h1>
            <nav>
                <a href="index.html">About</a>
                <a href="projects.html">Projects</a>
                <a href="blog.html">Blog</a>
            </nav>
        </header>

        <main>
            <section id="projects">
                <h2>Recent Projects</h2>
                <div class="project">
                    <h3>Attacks against AI systems</h3>
                    <p> I am interested in attacks that expose vulnerabilities of AI systems and quantify risks associated with deploying AI. Examples include work demonstrating how to <a href="https://arxiv.org/abs/2403.06634"> "steal" parts of production grade LLMs from public facing APIs </a>, quantifying robustness of the Gemini family of models to <a href="https://arxiv.org/pdf/2403.05530"> jailbreaks and prompt injection attacks </a>, and quantifying the <a href="https://arxiv.org/abs/1812.01647"> risk of catastrophic failure </a> in AI agents.</p>
                </div>
                <div class="project">
                    <h3>Human AI collaboration and Human Factors in Aligning AI</h3>
                    <p>Moder AI systems are often best used as assistants. This brings the question: How should AI be designed to best collaborate and communicate with Humans? What are the right modes of communication, and should communication be restricted to specific forms to best facilitate this collaboration? In a series of works from the past few years, we have made progress on understanding this, developing systems that <a href="https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/"> optimally integrate predictions from human clinicians and AI </a>for breast cancer and TB diagnosis, showing how <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20465"> human perception of the AI </a> and <a href="https://dl.acm.org/doi/abs/10.1145/3600211.3604692"> AI understanding of human uncertainty </a> affects collaborative performance, and studying the forms <a href="https://ojs.aaai.org/index.php/AIES/article/view/31637"> human feedback should take </a> when collecting data for AI alignment.</p>
                </div>
                <div class="project">
                    <h3>Certifiably Secure AI</h3>
                    <p>AI systems are increasingly deployed in agentic scenarios with access to sensitive information and the ability to take consequentail actions on behalf of a user. These deployments create serious privacy and security risks. Anticipating exactly what attackers may do here is hard as the possiblities increase, and this project seeks to develop mathematical guarantees on the worst case behavior of AI or AI-powered systems. A couple of recent examples is an approach to <a href="https://openreview.net/forum?id=HF8PmUAaFu"> certifying the robustness of learning algorithms against adaptive and dynamic data poisoning attacks </a>, and the development of <a href="https://arxiv.org/abs/2310.06771"> superior correlated noise mechanims for differentially private machine learning </a>. </p>
                </div>
            </section>
        </main>

        <footer>
            <p>&copy; 2024 Krishnamurthy (Dj) Dvijotham</p>
        </footer>
    </div>
</body>
</html>
