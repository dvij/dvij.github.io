I have worked on multiple aspects of evaluating, improving and formally guaranteeing security, privacy and reliability of deep learning enabled AI systems: 

1. My recent focus has been on safety/security for agentic AI systems. We recently built a modular plug-in security evaluation framework for agents compatible with any agentic framework ([browsergym](https://github.com/ServiceNow/BrowserGym), [tau-bench](https://github.com/sierra-research/tau-bench), ..). The paper and open source code associated with this project has been released at [this link]https://github.com/ServiceNow/DoomArena) - the associated paper has also been accepted to COLM 2025. We also built an internal red teaming tool exposing security vulnerabilities of ServiceNow's AI offerings and also worked on mitigations to address these

2. I have developed several attacks that expose previously unknown vulnerabilities of AI systems. The [most recent one](https://jlkazdan.github.io/NOICE/) involves stealthy fine tuning attacks that were validated against production fine tuning APIs of several leading LLM providers, including OpenAI and Anthropic - this work was awarded a bug bounty by OpenAI's security team. Previously, I contributed to work that showed that production grade LLM APIs can be used to [steal parts of closed source models](https://not-just-memorization.github.io/partial-model-stealing.html), that won a best paper award at ICML 2024.

3. I led the adversarial testing efforts ([report](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) Section 9)  for the Gemini 1.5 family of models, evaluating Gemini for vulnerabilities to jailbreaking and prompt injection attacks 

4. Previously, I worked a lot on provable robustness guarantees against adversarial attacks on deep learning systems, with some highlights being in [this framework](https://github.com/google-deepmind/jax_verify) (an earlier version of which was productionized on the Google play store to improve robustness of text classifiers for detecting fraudulent apps against adversarial attacks) and [this work](https://arxiv.org/abs/2206.10550) that was, at the time of publication, the SOTA in provable robustness guarantees against norm bounded adversarial attacks. We are currently working on extending these approaches to VLMs, particularly those that use inference time reasoning/thinking. 

5. I was a primary contributor to [this work](https://arxiv.org/abs/2404.16706) that achieves the current SOTA tradeoff between memory and performance on Google's differentially private federated learning system (see [here](https://arxiv.org/html/2408.08868v2)).

6. I have also worked on human-AI collaboration and its implications for alignment, including [this work](https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/) that showed state of the art results by combining Google's medical image AI systems with human clinicians. I also contributed to this work on [richer feedback for t2i models](https://openaccess.thecvf.com/content/CVPR2024/html/Liang_Rich_Human_Feedback_for_Text-to-Image_Generation_CVPR_2024_paper.html) that won a best paper at CVPR 2024.
